{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!kill $(lsof -t -i:7860)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGGR3Dma8XkV",
        "outputId": "a0979f32-649e-4974-cd7e-ebd8b441f04b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "-QU3FeicmOM8",
        "outputId": "72e9a1e9-efbf-4dc8-fe4d-95dffba80ca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4280712122.py:35: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4280712122.py:37: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Gradio frontend running at: NgrokTunnel: \"https://6c31e3405f2d.ngrok-free.app\" -> \"http://localhost:7860\"\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://de38b98f6451d4d347.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://de38b98f6451d4d347.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# ======================================================\n",
        "# ü§ñ AI Customer Support Bot (Groq + LangChain + Chroma + Gradio + ngrok)\n",
        "# ======================================================\n",
        "\n",
        "# Step 1: Install dependencies\n",
        "!pip install langchain langchain-core langchain-community langchain-groq gradio pypdf chromadb sentence_transformers nest_asyncio pyngrok > /dev/null\n",
        "\n",
        "# Step 2: Imports\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "import gradio as gr\n",
        "import tempfile\n",
        "import os\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Step 3: Apply nest_asyncio (needed for Colab)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Step 4: Set your ngrok auth token\n",
        "ngrok.set_auth_token(\"2yLZwyu0M6abrDAXgjcaaDqArlL_5PtJBVqHHxyu9upkUV3UH\")\n",
        "\n",
        "# Step 5: Initialize LLM\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    groq_api_key=\"gsk_...\",  # üîë Replace with your Groq API key\n",
        "    model_name=\"llama-3.3-70b-versatile\"\n",
        ")\n",
        "\n",
        "# Step 6: Global objects\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "vectorstore = None  # will hold Chroma vectorstore\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# ======================================================\n",
        "# Optional: Upload Knowledge Base (PDF)\n",
        "# ======================================================\n",
        "def upload_pdf(file):\n",
        "    global vectorstore\n",
        "    if file is None:\n",
        "        return \"‚ö†Ô∏è Please upload a PDF file.\"\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
        "        tmp.write(file.read())\n",
        "        tmp_path = tmp.name\n",
        "\n",
        "    loader = PyPDFLoader(tmp_path)\n",
        "    docs = loader.load()\n",
        "    vectorstore = Chroma.from_documents(docs, embedding_function)\n",
        "    os.remove(tmp_path)\n",
        "    return f\"‚úÖ Knowledge base '{file.name}' uploaded successfully!\"\n",
        "\n",
        "# ======================================================\n",
        "# Chatbot Function with Context Memory & Escalation\n",
        "# ======================================================\n",
        "def chat_with_bot(message, history):\n",
        "    global vectorstore, memory\n",
        "\n",
        "    if vectorstore:\n",
        "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "    else:\n",
        "        retriever = None\n",
        "\n",
        "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        memory=memory,\n",
        "        return_source_documents=False\n",
        "    )\n",
        "\n",
        "    response = qa_chain.invoke({\"question\": message})\n",
        "    answer = response[\"answer\"]\n",
        "\n",
        "    # Escalation simulation\n",
        "    if any(phrase in answer.lower() for phrase in [\"i don't know\", \"not sure\", \"no information\"]):\n",
        "        answer += \"\\n\\nüî∫ Escalating this query to a human support agent.\"\n",
        "\n",
        "    return answer\n",
        "\n",
        "# ======================================================\n",
        "# Launch Gradio UI with ngrok\n",
        "# ======================================================\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ü§ñ AI Customer Support Bot\")\n",
        "    gr.Markdown(\n",
        "        \"üí° Features:\\n\"\n",
        "        \"- Contextual memory of conversation\\n\"\n",
        "        \"- Escalation simulation for unanswered queries\\n\"\n",
        "        \"- Optional PDF knowledge base upload\"\n",
        "    )\n",
        "\n",
        "    kb_upload = gr.File(label=\"üìö Upload Knowledge Base (PDF)\")\n",
        "    upload_btn = gr.Button(\"Upload\")\n",
        "    upload_output = gr.Textbox(label=\"Upload Status\")\n",
        "\n",
        "    upload_btn.click(upload_pdf, inputs=kb_upload, outputs=upload_output)\n",
        "\n",
        "    gr.ChatInterface(fn=chat_with_bot, title=\"AI Support Chat\").render()\n",
        "\n",
        "# Step 1: Expose Gradio via ngrok\n",
        "public_url = ngrok.connect(7860)\n",
        "print(\"üöÄ Gradio frontend running at:\", public_url)\n",
        "\n",
        "# Step 2: Launch Gradio app\n",
        "demo.launch(server_name=\"0.0.0.0\", server_port=7860)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jgSsf-0ZyH62"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}